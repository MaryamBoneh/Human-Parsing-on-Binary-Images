{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self-Correction-Human-Parsing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5YZ9r5saZi0"
      },
      "outputs": [],
      "source": [
        "!pip install Ninja\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bQ45QDh5jf0q"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os, cv2, wandb\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnuiyB88UYd9"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/PeikeLi/Self-Correction-Human-Parsing.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "777sqDd6hgRp"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Healthcare-Robotics/bodies-at-rest.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"Self-Correction-Human-Parsing-Binary-Images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "cEzfM9Bvhra7",
        "outputId": "99800c90-95f0-4324-861a-2d33c425378d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220314_130617-2n0cfosk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/maryamboneh/Self-Correction-Human-Parsing-Binary-Images/runs/2n0cfosk\" target=\"_blank\">elderberry-square-1</a></strong> to <a href=\"https://wandb.ai/maryamboneh/Self-Correction-Human-Parsing-Binary-Images\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/maryamboneh/Self-Correction-Human-Parsing-Binary-Images/runs/2n0cfosk?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f7beef9ff50>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q8jfCufAACOd"
      },
      "outputs": [],
      "source": [
        "!mkdir Self-Correction-Human-Parsing-Segmented-Dataset\n",
        "!mkdir Self-Correction-Human-Parsing-Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuH2g2MaUdAU",
        "outputId": "3879446d-f42e-42d2-fe52-e37fab81c9b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Self-Correction-Human-Parsing\n"
          ]
        }
      ],
      "source": [
        "%cd Self-Correction-Human-Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6EPnpRFNVMn2"
      },
      "outputs": [],
      "source": [
        "!mkdir input\n",
        "!mkdir output\n",
        "!mkdir dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjyFUwGiU_qp",
        "outputId": "8ba77517-a28f-4b59-febc-35c6ad6ed49b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/bodies-at-rest/PressurePose\n"
          ]
        }
      ],
      "source": [
        "%cd /content/bodies-at-rest/PressurePose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "focJqFhAiQoJ"
      },
      "outputs": [],
      "source": [
        "!chmod +x download_real.sh\n",
        "!bash download_real.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3pkBlvug-2IW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebbd8ed9-1575-48e6-a635-be2caa75f348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48\n",
            "(880, 440, 3)\n"
          ]
        }
      ],
      "source": [
        "object = pd.read_pickle(r'/content/bodies-at-rest/data_BR/real/S103/prescribed.p')\n",
        "print(len(object['RGB']))\n",
        "print(object['RGB'][0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1gkvbz8qyJx0"
      },
      "outputs": [],
      "source": [
        "path = \"/content/bodies-at-rest/data_BR/real/\"\n",
        "folders = os.listdir(path)\n",
        "w, h = 880, 440\n",
        "\n",
        "for i, folder in enumerate(folders):\n",
        "  object = pd.read_pickle(r'/content/bodies-at-rest/data_BR/real/'+ folder +'/prescribed.p')\n",
        "  for j in range(0, len(object['RGB'])):\n",
        "    rgb_img = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "    rgb_img = object['RGB'][j]\n",
        "    rgb_img = Image.fromarray(rgb_img, 'RGB')\n",
        "    rgb_img.save(f'/content/Self-Correction-Human-Parsing-Dataset/{i}{j}.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVyPAveIW8nZ",
        "outputId": "abab89c3-9225-4bb1-e992-7fbd028e3e5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating total class number 7 with ['Background', 'Head', 'Torso', 'Upper Arms', 'Lower Arms', 'Upper Legs', 'Lower Legs']\n",
            "100% 914/914 [03:58<00:00,  3.83it/s]\n"
          ]
        }
      ],
      "source": [
        "!python '/content/Self-Correction-Human-Parsing/simple_extractor.py' --dataset 'pascal' --model-restore '/content/drive/MyDrive/models/exp-schp-201908270938-pascal-person-part.pth' --input-dir '/content/Self-Correction-Human-Parsing-Dataset' --output-dir '/content/Self-Correction-Human-Parsing-Segmented-Dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55EI5IImXP_s",
        "outputId": "f280b4a7-b12f-4cfc-cde9-3e39ed70a984"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "914"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "path = \"/content/Self-Correction-Human-Parsing-Segmented-Dataset/\"\n",
        "segmented_images = os.listdir(path)\n",
        "\n",
        "len(segmented_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSkjL6qfJVcL",
        "outputId": "3a558294-5a7b-4bcf-f70c-028226fb1fee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Self-Correction-Human-Parsing/dataset\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Self-Correction-Human-Parsing/dataset/\n",
        "!mkdir train_images\n",
        "!mkdir train_segmentations\n",
        "!mkdir val_images\n",
        "!mkdir val_segmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "wyu-0U9dSCUT"
      },
      "outputs": [],
      "source": [
        "train_id = open('train_id.txt', 'w')\n",
        "val_id = open('val_id.txt', 'w')\n",
        "\n",
        "for i ,image in enumerate(segmented_images):\n",
        "  img = cv2.imread(os.path.join(path, image))\n",
        "  img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  img_gray[img_gray > 0] = 255\n",
        "  img_gray[img_gray == 0] = 0\n",
        "  image_name = image.split('.')[0]\n",
        "  if i < 800:\n",
        "    cv2.imwrite(f\"train_images/{image_name}.jpg\", img_gray)\n",
        "    train_id.write(f\"{image_name}\")\n",
        "    train_id.write('\\n')\n",
        "  else:\n",
        "    cv2.imwrite(f\"val_images/{image_name}.jpg\", img_gray)\n",
        "    val_id.write(f\"{image_name}\")\n",
        "    val_id.write('\\n')\n",
        "\n",
        "train_id.close()\n",
        "val_id.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "x-P87X4ibHoD"
      },
      "outputs": [],
      "source": [
        "path = \"/content/Self-Correction-Human-Parsing-Segmented-Dataset/\"\n",
        "segmented_images = os.listdir(path)\n",
        "\n",
        "for i, image in enumerate(segmented_images):\n",
        "  img_rgb = cv2.imread(os.path.join(path, image))\n",
        "  img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  img_gray[img_gray == 0] = 0\n",
        "  img_gray[img_gray == 15] = 1\n",
        "  img_gray[img_gray == 38] = 2\n",
        "  img_gray[img_gray == 53] = 3\n",
        "  img_gray[img_gray == 75] = 4\n",
        "  img_gray[img_gray == 90] = 5\n",
        "  img_gray[img_gray == 113] = 6\n",
        "\n",
        "  if i < 800:\n",
        "    cv2.imwrite(f\"train_segmentations/{image}\", img_gray)\n",
        "  else:\n",
        "    cv2.imwrite(f\"val_segmentations/{image}\", img_gray)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGe1yKSapal7",
        "outputId": "1b6cf339-4c77-4154-c428-5194537e00ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Self-Correction-Human-Parsing\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Self-Correction-Human-Parsing/\n",
        "!mkdir pretrain_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "bAaGQ0oZyBqe",
        "outputId": "b0ab8040-f1dd-4ba4-ac8b-a5ed2a6c4a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet101-imagenet.pth\n",
            "To: /content/Self-Correction-Human-Parsing/pretrain_model/resnet101-imagenet.pth\n",
            "100%|██████████| 179M/179M [00:05<00:00, 30.4MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Self-Correction-Human-Parsing/pretrain_model/resnet101-imagenet.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "url = \"http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet101-imagenet.pth\"\n",
        "out = \"/content/Self-Correction-Human-Parsing/pretrain_model/resnet101-imagenet.pth\"\n",
        "gdown.download(url, out) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "d8FtT-Nsewdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461e588c-a5b0-4b87-f908-d80c9fb89eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaryamboneh\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/Self-Correction-Human-Parsing/wandb/run-20220314_132250-359vxtel\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstrawberry-brulee-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/maryamboneh/Self-Correction-Human-Parsing-Binary-Images\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/maryamboneh/Self-Correction-Human-Parsing-Binary-Images/runs/359vxtel\u001b[0m\n",
            "Namespace(arch='resnet101', batch_size=2, cycle_epochs=10, data_dir='/content/Self-Correction-Human-Parsing/dataset', epochs=10, eval_epochs=10, gpu='0,1,2', ignore_label=255, imagenet_pretrain='./pretrain_model/resnet101-imagenet.pth', input_size='473,473', lambda_c=0.1, lambda_e=1, lambda_s=1, learning_rate=0.007, log_dir='./log', model_restore='./log/checkpoint.pth.tar', momentum=0.9, num_classes=7, random_mirror=False, random_scale=False, schp_restore='./log/schp_checkpoint.pth.tar', schp_start=100, start_epoch=0, weight_decay=0.0005)\n",
            "image mean: [0.406, 0.456, 0.485]\n",
            "image std: [0.225, 0.224, 0.229]\n",
            "input space:BGR\n",
            "BGR Transformation\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Total training samples: 800\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "iter = 0 of 1330 completed, lr = 7.000000000000001e-05, loss = 3.767442226409912\n",
            "iter = 100 of 1330 completed, lr = 7.000000000000001e-05, loss = 2.24155855178833\n",
            "epoch = 0 of 10 completed using 339.06682415800014 s\n",
            "iter = 200 of 1330 completed, lr = 0.000763, loss = 0.8920587301254272\n",
            "epoch = 1 of 10 completed using 331.28270842150005 s\n",
            "iter = 300 of 1330 completed, lr = 0.001456, loss = 0.8113586902618408\n",
            "epoch = 2 of 10 completed using 328.7003708710001 s\n",
            "iter = 400 of 1330 completed, lr = 0.0021490000000000003, loss = 0.6672802567481995\n",
            "iter = 500 of 1330 completed, lr = 0.0021490000000000003, loss = 0.5313510298728943\n",
            "epoch = 3 of 10 completed using 327.37875298275003 s\n",
            "iter = 600 of 1330 completed, lr = 0.0028420000000000003, loss = 0.48276928067207336\n",
            "epoch = 4 of 10 completed using 326.580525674 s\n",
            "iter = 700 of 1330 completed, lr = 0.0035350000000000004, loss = 0.6652969121932983\n",
            "epoch = 5 of 10 completed using 326.0517923858333 s\n",
            "iter = 800 of 1330 completed, lr = 0.004228, loss = 0.6410250067710876\n",
            "iter = 900 of 1330 completed, lr = 0.004228, loss = 0.4891522526741028\n",
            "epoch = 6 of 10 completed using 325.6468718798572 s\n",
            "iter = 1000 of 1330 completed, lr = 0.004921, loss = 0.5582611560821533\n",
            "epoch = 7 of 10 completed using 325.363683882875 s\n",
            "iter = 1100 of 1330 completed, lr = 0.005614, loss = 0.4611055254936218\n",
            "epoch = 8 of 10 completed using 325.12866955488886 s\n",
            "iter = 1200 of 1330 completed, lr = 0.006307, loss = 0.521622359752655\n",
            "iter = 1300 of 1330 completed, lr = 0.006307, loss = 0.48097512125968933\n",
            "epoch = 9 of 10 completed using 325.0120374742 s\n",
            "Training Finished in 3250.120459176 seconds\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss █▇▆▅▄▃▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▁▂▁▂▂▂▁▁▂▁▁▁▁▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss 0.52325\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mstrawberry-brulee-3\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/maryamboneh/Self-Correction-Human-Parsing-Binary-Images/runs/359vxtel\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220314_132250-359vxtel/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python '/content/Self-Correction-Human-Parsing/train.py' --batch-size 2 --data-dir '/content/Self-Correction-Human-Parsing/dataset' --epochs 10 --num-classes 7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yskRtVlbl4ih"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}